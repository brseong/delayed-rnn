모든 연구의 기본
- 자기 연구와 겹치는 연구가 있는 지 찾아볼 것 (논문 찾아서 목록화 시켜야함.). GPT, Gemini (특히 deep research <- 논문 검색해서 보고서 써주는 AI. E.g. "SNN의 synaptic delay 개념을 RNN에 접목한 사례가 있나?"), Google scholar, Google.
- 자기 연구가 정말로 필요한 지 근거를 찾을 것.(논문 찾아서 목록화 시켜야함.)
표절 뿐만이 아니라, 다른 이유도 있는데, 논문이라는 건 보통, 통과하기 전에 피어 (peer) 리뷰를 받음. 이미 해당 학계에 있는 사람들이 이게 가치가 있는 논문인지 체크합니다. RNN을 한 30년 한 사람이 왔다. RNN 이거 안쓰고도 문제들을 해결할 수 있다! 우리 연구에 대해 저평가를 할 것.
-> 이 사람이 지적하기 전에, 논문에 미리 이 사람 연구를 인용해 놓습니다. 이 사람의 연구도 있고, 이 부분에서 이 사람의 연구가 유용하지만, 이런 부분에서는 우리가 문제삼은 부분을 해결하지는 못한다.

연구, 논문 -> 셀링 포인트 

저번 주 세미나에서 발표한 논문
- DELREC: LEARNING DELAYS IN RECURRENT SPIKING
NEURAL NETWORKS

- SNN에서, 이전 층(후두엽의 시각 피질)에서 이후 층(측두엽의 상황을 담당하는 피질)으로 전파되는 시간을 학습해 보자. 지연시간을 학습하자.

$x_1, x_2,\ldots, x_n \to h \to y_1, y_2, \ldots$

RNN 기반 모델의 고질적인 문제:
1. Long term dependency (장기 의존성), gradient vanishing (학습이 잘 안됨)
2. Queue 처리를 못함.

queue -> abcd 입력 abcd 출력

stack -> abcd 입력 dcba 출력

해결 방안: SNN처럼, 지연시간을 넣어보자.

되어 있는 작업: PS-MNIST
28*28 784 -> 순서를 바꿈 (어떻게 바꾸는지는 통일).
이거를 RNN에 넣고, 숫자를 예측해봐라.

해야 할 작업: Queue 입력받아, 순서 바꾸고 출력 (Queue + 내부 swap)